{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.7.9 64-bit ('lit-ada': conda)",
      "metadata": {
        "interpreter": {
          "hash": "cbba552011df5ccc8bc09bf1b64de4d563508c57d244dcff33d43b9b5d98878d"
        }
      }
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9-final"
    },
    "colab": {
      "name": "feature_visualization",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AE-C0zhrqFKF"
      },
      "source": [
        "# Feature Visualization\n",
        "\n",
        "(c) 2021 Fabian Offert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzGH8xvuqFKN"
      },
      "source": [
        "## Colab Setup\n",
        "\n",
        "Run the below commands only if you imported this notebook into Google Colab! Also **go to Runtime/Change runtime type and pick \"GPU\" as the hardware accelerator!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6xqAJSTqFKO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a373d43f-9aa1-40f7-f3d2-379e4b0006cc"
      },
      "source": [
        "!rm -rf minimal # In case this is re-run\n",
        "!git clone https://github.com/zentralwerkstatt/minimal\n",
        "!cp ./minimal/synset_words.txt ./"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'minimal'...\n",
            "remote: Enumerating objects: 10, done.\u001b[K\n",
            "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 10 (delta 0), reused 4 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (10/10), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svjWLikIqFKP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3463c023-6f64-4e29-91ce-0aae125f607b"
      },
      "source": [
        "!nvidia-smi # Check what kind of GPU we got"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu May  6 12:40:19 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    33W / 250W |   2099MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMgmBP85qFKQ"
      },
      "source": [
        "## Imports\n",
        "\n",
        "We are using PyTorch, the de-facto standard for high-level prototyping for machine learning. Because we are operating in high-dimensional vector space, we are also using Numpy, the Python library for scientific computing. Finally, we are importing several ready-to-use image filters, and some helper libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMei4NfjqFKQ"
      },
      "source": [
        "import torch as t\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision as tv\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from scipy.ndimage.filters import gaussian_filter, median_filter\n",
        "from skimage.restoration import denoise_bilateral, denoise_tv_chambolle\n",
        "import PIL.Image, PIL.ImageChops\n",
        "\n",
        "import os\n",
        "import random\n",
        "from io import BytesIO\n",
        "from IPython import display"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztxEiJ1kqFKQ"
      },
      "source": [
        "## Model to investigate\n",
        "\n",
        "This may take a while, as the pre-trained weights have to be loaded on first run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTwP3hjdqFKQ"
      },
      "source": [
        "device = t.device(\"cuda:0\" if t.cuda.is_available() else \"cpu\") # Use GPU if available\n",
        "f1 = tv.models.inception_v3(pretrained=True).to(device)\n",
        "f2 = tv.models.vgg16(pretrained=True).to(device)\n",
        "f3 = tv.models.vgg19(pretrained=True).to(device)\n",
        "# Test mode: we do not want to train the model (i.e. change its weights) at any point\n",
        "f1.eval()\n",
        "f2.eval()\n",
        "f3.eval()\n",
        "model_names = {'f1':'Inception V3', 'f2':'VGG16', 'f3': 'VGG19'}"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-A_Kto5MqFKR"
      },
      "source": [
        "## Helper functions\n",
        "\n",
        "Among other things, these helper functions allow us to convert between PyTorch tensors, NumPy arrays, and PIL images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkZJD9qKqFKR"
      },
      "source": [
        "# Show an image within a Jupyter environment\n",
        "# Can do PyTorch tensors, NumPy arrays, and PIL images\n",
        "def show_img(img, title='', fmt='jpeg'):\n",
        "    if type(img) is np.ndarray:\n",
        "        img = PIL.Image.fromarray(img)\n",
        "    elif type(img) is t.Tensor:\n",
        "        img = deprocess(img)\n",
        "    out = BytesIO()\n",
        "    if title: print(title)\n",
        "    img.save(out, fmt)\n",
        "    display.display(display.Image(data=out.getvalue()))\n",
        "\n",
        "# PyTorch is channels first, this happens here!\n",
        "preprocess = tv.transforms.Compose([tv.transforms.ToTensor()])\n",
        "    \n",
        "# Reverse of preprocess, PyTorch tensor to PIL image\n",
        "def deprocess(tensor):\n",
        "    # Clone tensor first, otherwise we are NOT making a copy by using .cpu()!\n",
        "    img = t.clone(tensor)\n",
        "    img = img.cpu().data.numpy().squeeze() # Get rid of batch dimension\n",
        "    img = img.transpose((1, 2, 0)) # Channels first to channels last\n",
        "    \n",
        "    # We are not using ImageNet images as input\n",
        "    # mean = np.array([0.485, 0.456, 0.406]) \n",
        "    # std = np.array([0.229, 0.224, 0.225]) \n",
        "    # img = std * img + mean\n",
        "\n",
        "    # No clipping, adversarial regulation should take care of this\n",
        "    # img = np.clip(img, 0, 1)\n",
        "    \n",
        "    # 0./1. range to 0./255. range\n",
        "    img *= 255\n",
        "    \n",
        "    img = img.astype(np.uint8)\n",
        "    img = PIL.Image.fromarray(img)\n",
        "    return img\n",
        "\n",
        "# Return a gray square PIL image\n",
        "def gray_square(size):\n",
        "    # Gray square, -1./1. range\n",
        "    img = np.random.normal(0, 0.01, (size, size, 3)) \n",
        "    \n",
        "    # -1./1. range to 0./255. range\n",
        "    img /= 2.\n",
        "    img += 0.5\n",
        "    img *= 255.\n",
        "\n",
        "    img = img.astype(np.uint8)\n",
        "    img = PIL.Image.fromarray(img)\n",
        "    return img\n",
        "\n",
        "# Load ImageNet classes\n",
        "with open('synset_words.txt') as synset_words_file:\n",
        "    synset_words = synset_words_file.readlines()\n",
        "for i, line in enumerate(synset_words):\n",
        "    synset_words[i] = line.replace(' ', '_').replace(',', '_').lower().strip()\n",
        "\n",
        "# Classify an image with the target model \n",
        "# Can do PyTorch tensors and PIL images\n",
        "def predict(img, model):\n",
        "    if type(img) is t.Tensor:\n",
        "        preds = model(img.to(device))\n",
        "    else:\n",
        "        preds = model(preprocess(img).unsqueeze(0).to(device))\n",
        "    preds_softmax_np = F.softmax(preds, dim=1).cpu().data.numpy()\n",
        "    # Returns class no., class name, and prediction confidence\n",
        "    return preds_softmax_np.argmax(), synset_words[preds_softmax_np.argmax()], preds_softmax_np.max()\n",
        "\n",
        "# \"Rolling\" list: whenever an item is added, the first item is discarded\n",
        "def destructive_append(l,i):\n",
        "    l=l[1:]\n",
        "    l.append(i)\n",
        "    return l\n",
        "\n",
        "# PyTorch and skimage use different channel ordering\n",
        "def pytorch_to_skimage(img):\n",
        "    # No batch dimension\n",
        "    img = img[0]\n",
        "    # Channels last\n",
        "    img = np.swapaxes(img, 0, 2)\n",
        "    return img\n",
        "    \n",
        "def skimage_to_pytorch(img):\n",
        "    # Channels first\n",
        "    img = np.swapaxes(img, 0, 2)\n",
        "    # Skimage uses double\n",
        "    img = img.astype(np.float32)\n",
        "    # No Batch dimension\n",
        "    img = np.expand_dims(img, 0)\n",
        "    return img\n",
        "\n",
        "# Filters for feature visualization\n",
        "def filter_median(npimg, params):\n",
        "    npimg = median_filter(npimg, size=(1, 1, params['fsize'], params['fsize']))  \n",
        "    return npimg\n",
        "\n",
        "def filter_bilateral(npimg, params):\n",
        "    npimg = pytorch_to_skimage(npimg)\n",
        "    npimg = denoise_bilateral(npimg, sigma_color=0.05, sigma_spatial=15, multichannel=True)\n",
        "    npimg = skimage_to_pytorch(npimg)\n",
        "    return npimg\n",
        "\n",
        "def filter_TV(npimg, params):\n",
        "    npimg = pytorch_to_skimage(npimg)\n",
        "    npimg = denoise_tv_chambolle(npimg, weight=0.1, multichannel=True)\n",
        "    npimg = skimage_to_pytorch(npimg)\n",
        "    return npimg"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJMdowgoqFKT"
      },
      "source": [
        "## Gradient Ascent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--8qv2LWqFKW"
      },
      "source": [
        "def gradient_ascent(img, neuron, model):\n",
        "\n",
        "    ITERATIONS = 2000\n",
        "    # FILTERS = [{'function':filter_median, 'frequency':4, 'params':{'fsize':5}}] # Good parameters\n",
        "    FILTERS = [{'function':filter_TV, 'frequency':20, 'params':{}}] # Good parameters\n",
        "    JITTER = 32\n",
        "    LR = 0.4\n",
        "    L2 = 1e-4 # Yosinski weight decay\n",
        "            \n",
        "    input = preprocess(img).unsqueeze(0).to(device).requires_grad_()\n",
        "    optimizer = t.optim.SGD([input], lr=LR, weight_decay=L2)\n",
        "    \n",
        "    for i in range(ITERATIONS):\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Centers the object in the image\n",
        "        if JITTER:\n",
        "            npimg = input.data.cpu().numpy() # To CPU and numpy\n",
        "            ox, oy = np.random.randint(-JITTER, JITTER+1, 2)\n",
        "            npimg = np.roll(np.roll(npimg, ox, -1), oy, -2) # Jitter\n",
        "            input.data = t.from_numpy(npimg).to(device)\n",
        "\n",
        "        x = model(input)\n",
        "        loss = -x[:,neuron]\n",
        "\n",
        "        preds_softmax_np = F.softmax(x, dim=1).cpu().data.numpy()\n",
        "        confidence = preds_softmax_np[:,neuron]\n",
        "                    \n",
        "        if i%50 == 0: \n",
        "            print(f'Iterations: {i}, loss: {loss.item()}, pred.: {synset_words[preds_softmax_np.argmax()]}, conf.: {confidence}')\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Centers the object in the image\n",
        "        if JITTER:\n",
        "            npimg = input.data.cpu().numpy() # To CPU and numpy\n",
        "            npimg = np.roll(np.roll(npimg, -ox, -1), -oy, -2) # Jitter\n",
        "            input.data = t.from_numpy(npimg).to(device)\n",
        "            \n",
        "        # Stochastic clipping\n",
        "        input.data[input.data > 1] = np.random.uniform(0, 1)\n",
        "        input.data[input.data < 0] = np.random.uniform(0, 1)\n",
        "        \n",
        "        # Filtering\n",
        "        for filter_ in FILTERS:\n",
        "            if i != ITERATIONS - 1: # No regularization on last iteration for good quality output\n",
        "                if i % filter_['frequency'] == 0:\n",
        "                    npimg = input.data.cpu().numpy() # To CPU and numpy\n",
        "                    npimg = filter_['function'](npimg, filter_['params'])\n",
        "                    input.data = t.from_numpy(npimg).to(device)\n",
        "        # Verbose\n",
        "        if i%50==0:\n",
        "          show_img((input))\n",
        "\n",
        "    return input"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "noise = gray_square(299)\n",
        "img = gradient_ascent(noise, 1, f1)"
      ]
    }
  ]
}
